{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiBWtgoXhddg",
        "outputId": "20ef0511-3dd8-4f68-9b17-7b9419dcf1e6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 916 kB/s \n",
            "\u001b[?25hRequirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 22.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n",
            "Collecting GetOldTweets3\n",
            "  Downloading GetOldTweets3-0.0.11-py3-none-any.whl (13 kB)\n",
            "Collecting pyquery>=1.2.10\n",
            "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from GetOldTweets3) (4.8.0)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: cssselect, pyquery, GetOldTweets3\n",
            "Successfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.3\n",
            "Collecting treeinterpreter\n",
            "  Downloading treeinterpreter-0.2.3-py2.py3-none-any.whl (6.0 kB)\n",
            "Installing collected packages: treeinterpreter\n",
            "Successfully installed treeinterpreter-0.2.3\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.27.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance\n",
        "!pip install GetOldTweets3\n",
        "!pip install treeinterpreter\n",
        "!pip install tweepy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import sys\n",
        "import re\n",
        "import string\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import tweepy"
      ],
      "metadata": {
        "id": "6SzmeKKUhgM6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import unicodedata\n",
        "sentiment_i_a = SentimentIntensityAnalyzer()\n",
        "\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *"
      ],
      "metadata": {
        "id": "TIG6k-5XhnL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from treeinterpreter import treeinterpreter as ti\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVR \n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "HVA1222dhtEt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get stock data for the given Stock Name\n",
        "def getStockDetails(stockname,start_time,end_time):\n",
        "  company = yf.Ticker(stockname)\n",
        "  company.info.get(\"longName\")\n",
        "  stockData = yf.download(stockname, start=start_time, end=end_time)\n",
        "  print(\"\\n Stock Data Obtained \")\n",
        "  print(stockData.head())\n",
        "  print(\"\\n\")\n",
        "  plt.title('Time series chart of Closing stocks for ' + company.info.get(\"longName\"))\n",
        "  plt.plot(stockData[\"Close\"])\n",
        "  plt.show()\n",
        "  print(\"\\n\")\n",
        "  stockData.to_csv('stockData_' + stockname + '.csv')\n",
        "  return stockData.head()\n"
      ],
      "metadata": {
        "id": "qS4eAeuuh01r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aQW9kNEl-GJA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method for data cleaning\n",
        "class TweetCleaner:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.punc_table = str.maketrans(\"\", \"\", string.punctuation) # to remove punctuation from each word in tokenize\n",
        "\n",
        "    def compound_word_split(self, compound_word):\n",
        "        matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', compound_word)\n",
        "        return [m.group(0) for m in matches]\n",
        "\n",
        "    def remove_non_ascii_chars(self, text):\n",
        "        return ''.join([w if ord(w) < 128 else ' ' for w in text])\n",
        "\n",
        "    def remove_hyperlinks(self,text):\n",
        "        return ' '.join([w for w in text.split(' ')  if not 'http' in w])\n",
        "\n",
        "    def get_cleaned_text(self, text):\n",
        "        cleaned_tweet = text.replace('\\\"','').replace('\\'','').replace('-',' ')\n",
        "        cleaned_tweet =  self.remove_non_ascii_chars(cleaned_tweet)\n",
        "        if re.match(r'RT @[_A-Za-z0-9]+:',cleaned_tweet):\n",
        "            cleaned_tweet = cleaned_tweet[cleaned_tweet.index(':')+2:]\n",
        "        cleaned_tweet = self.remove_hyperlinks(cleaned_tweet)\n",
        "        cleaned_tweet = cleaned_tweet.replace('#','HASHTAGSYMBOL').replace('@','ATSYMBOL') # to avoid being removed while removing punctuations\n",
        "        tokens = [w.translate(self.punc_table) for w in word_tokenize(cleaned_tweet)] # remove punctuations and tokenize\n",
        "        tokens = [nltk.WordNetLemmatizer().lemmatize(w) for w in tokens if not w.lower() in self.stop_words and len(w)>1] # remove stopwords and single length words\n",
        "        cleaned_tweet = ' '.join(tokens)\n",
        "        cleaned_tweet = cleaned_tweet.replace('HASHTAGSYMBOL','#').replace('ATSYMBOL','@')\n",
        "        cleaned_tweet = cleaned_tweet\n",
        "        return cleaned_tweet\n",
        "\n",
        "    def clean_tweets(self, tweets, is_bytes = False):   \n",
        "        test_tweet_list = []\n",
        "        for tweet in tweets:\n",
        "            if is_bytes:\n",
        "                test_tweet_list.append(self.get_cleaned_text(ast.literal_eval(tweet).decode(\"UTF-8\")))\n",
        "            else:\n",
        "                test_tweet_list.append(self.get_cleaned_text(tweet))\n",
        "        return test_tweet_list\n",
        "    \n",
        "    def clean_single_tweet(self, tweet, is_bytes = False):  \n",
        "        if is_bytes:\n",
        "             return self.get_cleaned_text(ast.literal_eval(tweet).decode(\"UTF-8\"))\n",
        "        return self.get_cleaned_text(tweet)\n",
        "    \n",
        "    def cleaned_file_creator(self, op_file_name, value1, value2):\n",
        "        csvFile = open(op_file_name, 'w+')\n",
        "        csvWriter = csv.writer(csvFile)\n",
        "        for tweet in range(len(value1)):\n",
        "            csvWriter.writerow([value1[tweet], value2[tweet]])\n",
        "        csvFile.close()"
      ],
      "metadata": {
        "id": "ibr1EXYph4qm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method for fetching tweets\n",
        "def fetchTweets(stockname,start_time,end_time):\n",
        "\n",
        "  consumer_key    = 'NQWkNwwCkZRUh8SFLvTlwZ1s1'\n",
        "  consumer_secret = 'fUSFCAQedFZyU0OkEZ3XbTyJZu3hLcs17t1aCzGcrOIu34ZSLE'\n",
        "\n",
        "  access_token  = '3218970096-Pi63aOvhmVuxBBeTJDgXg40SFwINEKE4qFBzy0Z'\n",
        "  access_token_secret = 'XXTftsCTGOMH0xAFtkr79rT80YntS02bmCBcXpuRxoxde'\n",
        "\n",
        "\n",
        "  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_token_secret)\n",
        "  api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "  data  = pd.DataFrame()\n",
        "  date_now=datetime.datetime.now().date()\n",
        "  date_now\n",
        "  date_now=datetime.datetime.now().date()\n",
        " \n",
        "  for i in range(7):\n",
        "    date_now =  date_now- datetime.timedelta(days=1)\n",
        "    print(date_now)\n",
        "    fetch_tweets=tweepy.Cursor(api.search, q=\"$AAPL\",count=100, lang =\"en\",until=date_now, tweet_mode=\"extended\").items(3)\n",
        "    data_new=pd.DataFrame(data=[[tweet_info.created_at.date(),tweet_info.full_text]for tweet_info in fetch_tweets],columns=['Date','Tweets'])\n",
        "    data=data.append(data_new)\n",
        "  \n",
        "  data[1:].to_csv(\"Tweets.csv\")\n",
        "  df= pd.read_csv(\"Tweets.csv\")\n",
        "  data=df.drop(columns= [\"Unnamed: 0\"] )\n",
        "\n",
        "\n",
        "  cdata=pd.DataFrame(columns=['Date','Tweets'])\n",
        "  total=100\n",
        "  index=0\n",
        "  for index,row in data.iterrows():\n",
        "    stre=row[\"Tweets\"]\n",
        "    my_new_string = re.sub('[^ a-zA-Z0-9]', '', stre)\n",
        "    temp_df = pd.DataFrame([[data[\"Date\"].iloc[index], my_new_string]], columns = ['Date','Tweets'])\n",
        "    print(temp_df)\n",
        "    cdata = pd.concat([cdata, temp_df], axis = 0).reset_index(drop = True)\n",
        "    # index=index+1\n",
        "  #print(cdata.dtypes)\n",
        "  cdata.to_csv('tweets_' + stockname  + '.csv')\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "YqDQ9IXuiBZB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method for processing tweets\n",
        "def processTweets(stockname):\n",
        "  columns=['Date','Tweets']\n",
        "  data = pd.DataFrame(columns)\n",
        "  df = pd.read_csv('tweets_' + stockname  + '.csv',encoding='utf-8', names=columns, header=None)\n",
        "  print(df)\n",
        "  \n",
        "  indx=0\n",
        "  get_tweet=\"\"\n",
        "  #get tweets day wise\n",
        "  for i in range(0,len(df)-1):\n",
        "    get_date=df.Date.iloc[i]\n",
        "    next_date=df.Date.iloc[i+1]\n",
        "    if(str(get_date)==str(next_date)):\n",
        "      get_tweet = get_tweet + df.Tweets.iloc[i]+\" \"\n",
        "    if(str(get_date)!=str(next_date)):\n",
        "      data.at[indx,'Date'] = get_date\n",
        "      data.at[indx,'Tweets'] = get_tweet\n",
        "      indx=indx+1\n",
        "      get_tweet=\" \"\n",
        "\n",
        "  #get respective prices for each day using stockData\n",
        "  data['Prices']=\"\"\n",
        "  readStockData = pd.read_csv('stockData_' + stockname + '.csv')\n",
        "  readStockData.columns = [c.replace(' ', '_') for c in readStockData.columns]\n",
        "  for i in range (0,len(data)):\n",
        "      for j in range (0,len(readStockData)):\n",
        "          get_tweet_date = data.Date.iloc[i]\n",
        "          get_stock_date = readStockData.Date.iloc[j]\n",
        "          if(str(get_stock_date)==str(get_tweet_date)):\n",
        "            data.at[i,'Prices'] = int(readStockData.Adj_Close[j])\n",
        "            break\n",
        "\n",
        "  #drop rows that do not have Price values\n",
        "  data['Prices'].replace('', np.nan, inplace=True)\n",
        "  data.dropna(subset=['Prices'], inplace=True)\n",
        "  data.reset_index(drop=True, inplace=True)\n",
        "  data['Prices'] = data['Prices'].apply(np.int64)\n",
        "  data.drop(0, 1, inplace=True)\n",
        "  print(data.head())\n",
        "  data.to_csv('processedTweets_' + stockname  + '.csv')"
      ],
      "metadata": {
        "id": "al1IeGIMiD2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method for sentiment analysis for tweets\n",
        "def sentimentAnalysis(stockname):\n",
        "  data = pd.read_csv('processedTweets_' + stockname  + '.csv', encoding='utf-8')\n",
        "  data[\"Comp\"] = ''\n",
        "  data[\"Negative\"] = ''\n",
        "  data[\"Neutral\"] = ''\n",
        "  data[\"Positive\"] = ''\n",
        "  for indexx, row in data.T.iteritems():\n",
        "    try:\n",
        "      sentence_i = unicodedata.normalize('NFKD', data.loc[indexx, 'Tweets'])\n",
        "      sentence_sentiment = sentiment_i_a.polarity_scores(sentence_i)\n",
        "      data.at[indexx, 'Comp'] =  sentence_sentiment['compound']\n",
        "      data.at[indexx, 'Negative'] = sentence_sentiment['neg']\n",
        "      data.at[indexx, 'Neutral'] =  sentence_sentiment['neu']\n",
        "      data.at[indexx, 'Positive'] = sentence_sentiment['pos'] \n",
        "    except TypeError:\n",
        "      print('failed on_status,',str(e))\n",
        "\n",
        "  data.drop(['Unnamed: 0'], 1, inplace=True)\n",
        "  print(data.head())\n",
        "  data.to_csv('sentimentAnalysis_' + stockname  + '.csv')\n",
        "\n",
        "  posi=0\n",
        "  nega=0\n",
        "  neutral = 0\n",
        "  for i in range (0,len(data)):\n",
        "    get_val = data.Comp[i]\n",
        "    if(float(get_val)<(0)):\n",
        "        nega=nega+1\n",
        "    if(float(get_val>(0))):\n",
        "        posi=posi+1\n",
        "    if(float(get_val)==(0)):\n",
        "        neutral=neutral+1\n",
        "  \n",
        "  posper=(posi/(len(data)))*100\n",
        "  negper=(nega/(len(data)))*100\n",
        "  neutralper=(neutral/(len(data)))*100\n",
        "\n",
        "  arr=np.asarray([posper,negper,neutralper], dtype=int)\n",
        "  plt.figure()\n",
        "  plt.pie(arr,labels=['positive','negative', 'neutral'])\n",
        "  plt.plot()\n",
        "\n",
        "  print(\"% of positive tweets= \",posper)\n",
        "  print(\"% of negative tweets= \",negper)\n",
        "  print(\"% of neutral tweets= \",neutralper)"
      ],
      "metadata": {
        "id": "oYzmScmeiHAf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method for stock price prediction using random forest model\n",
        "def RandomForestModel(stockname):\n",
        "  df = pd.read_csv('sentimentAnalysis_' + stockname  + '.csv', encoding='utf-8')\n",
        "  train, test = train_test_split(df, shuffle=False, test_size=0.5)\n",
        "  \n",
        "  sentiment_score_list_train = []\n",
        "  for date, row in train.T.iteritems():\n",
        "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
        "    sentiment_score_list_train.append(sentiment_score)\n",
        "  numpy_df_train = np.asarray(sentiment_score_list_train)\n",
        "\n",
        "  sentiment_score_list_test = []\n",
        "  for date, row in test.T.iteritems():\n",
        "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
        "    sentiment_score_list_test.append(sentiment_score)\n",
        "  numpy_df_test = np.asarray(sentiment_score_list_test)\n",
        "\n",
        "  y_train = pd.DataFrame(train['Prices'])\n",
        "  y_test = pd.DataFrame(test['Prices'])\n",
        "\n",
        "\n",
        "  rf = RandomForestRegressor()\n",
        "  rf.fit(numpy_df_train, y_train)\n",
        "  prediction, bias, contributions = ti.predict(rf, numpy_df_test)\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "  plt.figure()\n",
        "  plt.plot(test['Prices'].iloc[:].values)\n",
        "  plt.plot(prediction.flatten())\n",
        "  plt.title('Random Forest predicted prices')\n",
        "  plt.ylabel('Stock Prices')\n",
        "  plt.xlabel('Days')\n",
        "  plt.legend(['actual', 'predicted'])\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"RMSE value for Random Forest Model : \")\n",
        "  rmse = sqrt(mean_squared_error(y_test, prediction.flatten()))\n",
        "  print(rmse)\n",
        "  print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "id": "Z6oDik0AiJrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method for stock price prediction using support vector regression model\n",
        "def SVRModel(stockname):\n",
        "  df = pd.read_csv('sentimentAnalysis_' + stockname  + '.csv', encoding='utf-8')\n",
        "  train, test = train_test_split(df, shuffle=False, test_size=0.5)\n",
        "\n",
        "  sentiment_score_list_train = []\n",
        "  for date, row in train.T.iteritems():\n",
        "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
        "    sentiment_score_list_train.append(sentiment_score)\n",
        "  numpy_df_train = np.asarray(sentiment_score_list_train)\n",
        "\n",
        "  sentiment_score_list_test = []\n",
        "  for date, row in test.T.iteritems():\n",
        "    sentiment_score = np.asarray([df.loc[date, 'Negative'],  df.loc[date, 'Neutral'], df.loc[date, 'Positive']])\n",
        "    sentiment_score_list_test.append(sentiment_score)\n",
        "  numpy_df_test = np.asarray(sentiment_score_list_test)\n",
        "\n",
        "  y_train = pd.DataFrame(train['Prices'])\n",
        "  y_test = pd.DataFrame(test['Prices'])\n",
        "\n",
        "  svr_rbf = SVR(kernel='rbf', C=1e6, gamma=0.1)\n",
        "  svr_rbf.fit(numpy_df_train, y_train.values.flatten())\n",
        "  output_test_svm = svr_rbf.predict(numpy_df_test)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(test['Prices'].iloc[:].values)\n",
        "  plt.plot(output_test_svm)\n",
        "  plt.title('SVM predicted prices')\n",
        "  plt.ylabel('Stock Prices')\n",
        "  plt.xlabel('Days')\n",
        "  plt.legend(['actual', 'predicted'])\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "  print(\"RMSE value for Support Vector Regression Model : \")\n",
        "  rmse = sqrt(mean_squared_error(y_test, output_test_svm))\n",
        "  print(rmse)\n",
        "  print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "7VE3WBeyiNzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  name = input(\"Enter a valid STOCKNAME of the Corporation: \") #enter the name of the company\n",
        "  start_date = input(\"Enter the Start Date in the following format[YYYY-MM-DD]: \") #enter the start date to fetch the tweets\n",
        "  end_date = input(\"Enter the End Date in the following format[YYYY-MM-DD]: \" ) #enter the end date to fetch the tweets\n",
        "  \n",
        "  if(len(name) > 0):\n",
        "    STOCKNAME  = name\n",
        "  else:\n",
        "    STOCKNAME = \"AAPL\"\n",
        "  \n",
        "  if(len(start_date) > 0):\n",
        "    start_time = start_date\n",
        "  else:\n",
        "    start_time = \"2018-01-01\"\n",
        "  \n",
        "  if(len(end_date) > 0):\n",
        "    end_time = end_date\n",
        "  else:\n",
        "    end_time = \"2019-12-31\"\n",
        "\n",
        "\n",
        "  #Get Stock Details\n",
        "  print(\"------------------------------ Getting Stock details -----------------------------\")\n",
        "  stockData = getStockDetails(STOCKNAME,start_time,end_time)\n",
        "  print(\"Stock Details fetched! \\n\")\n",
        "\n",
        "  #Fetching tweets\n",
        "  print(\"------------------------------ Fetching Tweets -----------------------------\")\n",
        "  fetchTweets(STOCKNAME,start_time,end_time)\n",
        "  print(\"Tweets fetched! \\n\")\n",
        "\n",
        "  #Get tweets Per Day and get the stock closing values for each date\n",
        "  print(\"------------------------------ Processing Tweets -----------------------------\")\n",
        "  processTweets(STOCKNAME)\n",
        "  print(\"Processed Tweets ! \\n\")\n",
        "\n",
        "  #Perform Sentiment Analysis\n",
        "  print(\"------------------------------ Performing Sentiment Analysis -----------------------------\")\n",
        "  sentimentAnalysis(STOCKNAME)\n",
        "  print(\"Completed Sentiment Analysis on Tweets ! \\n\\n\")\n",
        "  #time.sleep(10);\n",
        "\n",
        "  #Training and Predicting using Random Forest Regression Model\n",
        "  print(\"--------  Training and Predicting using Random Forest Regression Model -------\")\n",
        "  RandomForestModel(STOCKNAME)\n",
        "  print(\"\\n \\n\")\n",
        "\n",
        "  #Training and Predicting using Support Vecor Regression Model\n",
        "  print(\"-------- Training and Predicting using Support Vector Regression Model ------------\")\n",
        "  SVRModel(STOCKNAME)\n",
        "  print(\"\\n \\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dxi1ZKByiQar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_ngrok\n",
        "!pip install pyngrok==4.1.1\n"
      ],
      "metadata": {
        "id": "aRdqjPVE55hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok   \n",
        "              "
      ],
      "metadata": {
        "id": "T_-Wh7xafXvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 28IRwFe7bRUwbQobgbJUZccvI4T_7gES7tSo5jvU8yfxSXeYy"
      ],
      "metadata": {
        "id": "7JJJUqPDf6g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask.globals import request\n",
        "from flask import Flask,render_template\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "from nk import getStockDetails\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "run_with_ngrok(app)\n",
        "@app.route('/')\n",
        "def home():\n",
        "  return render_template(\"index.html\")\n",
        "  \n",
        "  #return f\"<form><html>StockName:<input name='stock'> {{stock}}</html></form>\"\n",
        "\n",
        "\n",
        "@app.route('/main',methods= [\"GET\",\"POST\"])\n",
        "def tet():\n",
        "  if request.method=='POST':\n",
        "    stock=request.form.get(\"stock\")\n",
        "  return render_template('index.html',val=getStockDetails(stock,\"2022-04-16\",\"2022-04-24\"))\n",
        "\n",
        "app.run()"
      ],
      "metadata": {
        "id": "Qx7JU1Eoa2Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P0zQYRPwa_hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8NlWOhCAi8hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "L6hrkNiCi9TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('templates')"
      ],
      "metadata": {
        "id": "A96_Wsd2n-Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('static')"
      ],
      "metadata": {
        "id": "3h5wWuyroF_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ujOarCz8oWLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}